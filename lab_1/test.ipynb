{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:45:59.754688200Z",
     "start_time": "2024-03-14T06:45:59.675089700Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from lxml import etree\n",
    "import re\n",
    "import pathlib\n",
    "import flask\n",
    "import jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:45:59.775726300Z",
     "start_time": "2024-03-14T06:45:59.693470700Z"
    }
   },
   "outputs": [],
   "source": [
    "requests.adapters.DEFAULT_RETRIES =5\n",
    "# os.environ['CURL_CA_BUNDLE'] = ''\n",
    "doc_path = r'C:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\html_doc_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:45:59.775726300Z",
     "start_time": "2024-03-14T06:45:59.710292300Z"
    }
   },
   "outputs": [],
   "source": [
    "lsdir = os.listdir('.')\n",
    "if 'html_doc' not in lsdir:\n",
    "    os.mkdir('./html_doc_')\n",
    "if 'dataset' not in lsdir:\n",
    "    os.mkdir('./dataset')\n",
    "os.chmod('./html_doc_',0o777)\n",
    "aim = 'hug_dataset'\n",
    "html_path = os.path.join(os.getcwd(),'html_doc_')\n",
    "web_path = os.path.join(os.getcwd(),'dataset_')\n",
    "viewer_path = os.path.join(os.getcwd(),'viewer_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_DOC = dict()\n",
    "dir = os.listdir(doc_path)\n",
    "for k in dir:\n",
    "    with open(os.path.join(doc_path,k),'r',encoding='utf-8') as f:\n",
    "        HTML_DOC[k] = f.read()\n",
    "# for k,v in HTML_DOC.items():\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:46:22.779648800Z",
     "start_time": "2024-03-14T06:46:22.761286200Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_page_link(html_doc):\n",
    "    \"\"\"\n",
    "    页面链接\n",
    "    \"\"\"\n",
    "    soup = bs(html_doc,'lxml')\n",
    "    label = soup.find('h1',attrs={\n",
    "        'class':\"flex flex-wrap items-center leading-tight mb-3 text-lg md:text-xl\"\n",
    "                                       })\n",
    "    inner = label.contents\n",
    "    page_link = ''\n",
    "    err = '!!!'\n",
    "    # try:\n",
    "    #     page_link += inner[0].span.string + ' '\n",
    "    # except Exception:\n",
    "    #     page_link += err\n",
    "    try:\n",
    "        page_link += inner[2].a.string + ' '\n",
    "    except Exception:\n",
    "        page_link += err\n",
    "    try:\n",
    "        page_link += inner[4].a.string + ' '\n",
    "    except Exception:\n",
    "        page_link += err\n",
    "    try:\n",
    "        page_link += inner[6].find_all('button')[1].string + ' '\n",
    "    except Exception:\n",
    "        page_link += err\n",
    "    return page_link\n",
    "\n",
    "def find_download_time(html_doc):\n",
    "    soup = bs(html_doc,'lxml')\n",
    "    # print(soup.html.body.div.main.find_all('div')[2])\n",
    "\n",
    "    dom = etree.HTML(html_doc)\n",
    "    download_time = -1\n",
    "    try:\n",
    "        download_time = dom.xpath('/html/body/div/main/div[2]/section[2]/dl/dd/text()')[0]\n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "    return download_time\n",
    "\n",
    "\n",
    "\n",
    "# def find_dataset_numb(html_doc):\n",
    "#     soup = bs(html_doc,'lxml')\n",
    "#     dom = etree.HTML(html_doc)\n",
    "#     dataset_numb = -1\n",
    "#     try:\n",
    "#         dataset_numb = dom.xpath('/html/body/div/main/div[2]/section[2]/div[4]/a[6]/div[2]/text()')[1]\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     # print(dataset_numb)\n",
    "#     return dataset_numb\n",
    "\n",
    "def find_dataset_numb(html_doc):\n",
    "    index_1 = html_doc.rfind('Number of rows')\n",
    "    # print(index_1)\n",
    "    # print(html_doc[index_1:index_1+100])\n",
    "    index_2 = html_doc[index_1:].find('<!-- HTML_TAG_START -->') + len('<!-- HTML_TAG_START -->')\n",
    "    # print(html_doc[index_1:][index_2:index_2+100])\n",
    "    index_3 = html_doc[index_1:].find('<!-- HTML_TAG_END -->')\n",
    "    ret = html_doc[index_1:][index_2:index_3]\n",
    "    if len(ret) == 0:\n",
    "        ret = '-1'\n",
    "    return ret\n",
    "\n",
    "\n",
    "def find_dataset_size(html_doc:str):\n",
    "    index_1 = html_doc.rfind('Size of the auto-converted Parquet files')\n",
    "    index_2 = html_doc[index_1:].find('<!-- HTML_TAG_START -->') + len('<!-- HTML_TAG_START -->')\n",
    "    index_3 = html_doc[index_1:].find('<!-- HTML_TAG_END -->')\n",
    "    re = html_doc[index_1:][index_2:index_3]\n",
    "    if 'B' in re:\n",
    "        re_num = eval(re[:re.find('B')-1])\n",
    "        if 'K' in re:\n",
    "            re_num = re_num / 1024\n",
    "        elif 'M' in re:\n",
    "            re_num = re_num\n",
    "        elif 'G' in re:\n",
    "            re_num = re_num * 1024\n",
    "        elif 'T' in re:\n",
    "            re_num = re_num * 1024 * 1024\n",
    "        else:\n",
    "            re_num = re_num / (1024 * 1024)\n",
    "        return re_num\n",
    "    else:\n",
    "        return -1\n",
    "p = 0\n",
    "def find_dataset_viewer(file_name,html_doc):\n",
    "    global p\n",
    "    soup = bs(html_doc,'lxml')\n",
    "    dom = etree.HTML(html_doc)\n",
    "    try:\n",
    "        try:\n",
    "            # dataset_numb = soup.html.body.div.main.find_all('div')[2]\n",
    "            dataset_numb = dom.xpath('/html/body/div/main/div[2]/section[1]/div[5]/div')[0]\n",
    "            # text = etree.tostring(dataset_numb)\n",
    "            # soup_ = bs(text,'lxml').html.body\n",
    "            # p = soup_\n",
    "            # writeinner = soup_.get_text()\n",
    "            # text = re.sub(r'[\\r\\n]+', '\\n', writeinner)\n",
    "            # with open(os.path.join(viewer_path,file_name+'.html'),'bw') as f:\n",
    "            #     # f.write('method_1')\n",
    "            #     f.write(text.encode('utf-8'))\n",
    "            # print(1)\n",
    "\n",
    "                \n",
    "        except Exception:\n",
    "            # print(2)\n",
    "            dataset_numb = dom.xpath('/html/body/div/main/div[2]/section[1]/div[4]')[0]\n",
    "        \n",
    "        \n",
    "        text = etree.tostring(dataset_numb)\n",
    "        soup_ = bs(text,'lxml').html.body\n",
    "        p = soup_\n",
    "        writeinner = soup_.get_text()\n",
    "        text = re.sub(r'[\\r\\n]+', '\\n', writeinner)\n",
    "        with open(os.path.join(viewer_path,file_name+'.html'),'bw') as f:\n",
    "            # f.write('method_1')\n",
    "            f.write(text.encode('utf-8'))\n",
    "        \n",
    "            \n",
    "    except Exception:\n",
    "        pass\n",
    "    #     with open(os.path.join(viewer_path,file_name+'.html'),'wb') as f:\n",
    "    #     # for data in dataset_numb:\n",
    "    #         f.write('ERROR'.encode('utf-8'))\n",
    "    return(os.path.join(viewer_path,file_name+'.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:46:24.402111300Z",
     "start_time": "2024-03-14T06:46:24.396454500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "page_link: DIBT 10k_prompts_ranked 84 \n",
      "download_time: 1,219\n",
      "dataset_size: 3.58 MB\n",
      "dataset_numb: 10,331\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\DIBT_10k_prompts_ranked · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: liuhaotian LLaVA-Instruct-150K 299 \n",
      "download_time: 203\n",
      "dataset_size: -1 MB\n",
      "dataset_numb: -1\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\liuhaotian_LLaVA-Instruct-150K · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: Locutusque UltraTextbooks-2.0 20 \n",
      "download_time: 183\n",
      "dataset_size: 6287.36 MB\n",
      "dataset_numb: 3,220,278\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\Locutusque_UltraTextbooks-2.0 · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: maharshipandya spotify-tracks-dataset 46 \n",
      "download_time: 1,082\n",
      "dataset_size: 13.6 MB\n",
      "dataset_numb: 114,000\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\maharshipandya_spotify-tracks-dataset · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: nyanko7 danbooru2023 116 \n",
      "download_time: 78\n",
      "dataset_size: -1 MB\n",
      "dataset_numb: -1\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\nyanko7_danbooru2023 · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: PetraAI PetraAI 10 \n",
      "download_time: 16\n",
      "dataset_size: -1 MB\n",
      "dataset_numb: -1\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\PetraAI_PetraAI · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: ProgramComputer voxceleb 15 \n",
      "download_time: 22\n",
      "dataset_size: -1 MB\n",
      "dataset_numb: -1\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\ProgramComputer_voxceleb · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n",
      "page_link: speechcolab gigaspeech 53 \n",
      "download_time: 86,205\n",
      "dataset_size: -1 MB\n",
      "dataset_numb: -1\n",
      "viewer_path c:\\Users\\Elysia\\Desktop\\learn\\课件\\二下\\python\\lab_1\\viewer_\\speechcolab_gigaspeech · Datasets at Hugging Face.html.html\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('--------------------------------')\n",
    "for k,v in HTML_DOC.items():\n",
    "    # print('k',k)\n",
    "    page_link = find_page_link(v)\n",
    "    download_time = find_download_time(v)\n",
    "    dataset_size = find_dataset_size(v)\n",
    "    dataset_numb = find_dataset_numb(v)\n",
    "    viewer_paths = find_dataset_viewer(k,v)\n",
    "    print('page_link:',page_link)\n",
    "    print('download_time:',download_time)\n",
    "    print('dataset_size:',dataset_size,'MB')\n",
    "    print('dataset_numb:',dataset_numb)\n",
    "    print('viewer_path',viewer_paths)\n",
    "    print('--------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
