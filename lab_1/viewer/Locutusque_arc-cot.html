
		Augmented ARC-Challenge Dataset with Chain-of-Thought Reasoning
	
		Dataset Description
	
This dataset was created by augmenting the train subset of the AI2 Reasoning Challenge (ARC) dataset with chain-of-thought reasoning generated by Google's Gemini Pro language model. The goal is to provide additional context and intermediate reasoning steps to help models better solve the challenging multiple-choice science questions in ARC.
		Dataset Structure
	
The dataset contains 1068 training examples, with the following features:
question (string): The natural language science question. 
answer (string): The correct answer to the question.
		Dataset Creation
	
The chain-of-thought reasoning for each question-answer pair was generated using Google's Gemini Pro model. The model was given each question and the correct answer, and prompted to provide a detailed chain of reasoning for why that answer is correct. The generated chains of thought aim to break down the reasoning process into clear steps, providing additional context and explanations.
The train split of the ARC-Challenge dataset was used as the base, which contains 1068 multiple-choice science questions covering topics like physics, chemistry, biology, and earth science. The questions are generally at a 3rd-9th grade level.
		Intended Use
	
This dataset is intended to be used as a resource to train question answering models on reasoning about science questions. By providing the intermediate reasoning steps, the hope is that models can learn to reason more effectively and transparently about complex questions.
Potential use cases include:
Benchmarking question answering models on science reasoning
Analyzing the types of reasoning required for science QA
Improving model interpretability by generating reasoning traces
Studying few-shot learning with in-context chain-of-thought examples
		Limitations and Ethical Considerations
	
The chains of thought are generated by an AI system and may not always be entirely accurate or complete. They should be viewed as a supplemental learning resource rather than guaranteed perfect reasoning.
Additionally, the underlying ARC-Challenge questions may contain some social biases, as they are drawn from real-world science exams. Users should be aware of potential biases when training on this data.
		Dataset Specs
	
Number of examples: 1,068
Dataset size: 472 KB
Format: parquet
